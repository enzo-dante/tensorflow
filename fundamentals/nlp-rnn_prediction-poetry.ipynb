{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"nlp-rnn_prediction-poetry","provenance":[],"collapsed_sections":[],"authorship_tag":"ABX9TyMrqSXvdIOLVCoq1seOSYAG"},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","metadata":{"id":"F8YkC1zDUmda","colab_type":"text"},"source":["# predicting words \n","\n","Given a character, or a sequence of characters, what is the most probable next character? \n","\n","Since RNNs maintain an internal state that depends on the previously seen elements, given all the characters computed until this moment, what is the next character?\n","\n","The input to the model will be a sequence of characters, and we train the model to predict the outputâ€”the following character at each time step.\n"]},{"cell_type":"markdown","metadata":{"id":"NgXWsGVTR1b4","colab_type":"text"},"source":["# import libraries"]},{"cell_type":"code","metadata":{"id":"qQ3zkW5USKVI","colab_type":"code","outputId":"da4cae08-0bcd-4a8e-8fa2-6416507467a0","executionInfo":{"status":"ok","timestamp":1586264101464,"user_tz":240,"elapsed":1291,"user":{"displayName":"Enzo Vernon","photoUrl":"","userId":"09908679712030615005"}},"colab":{"base_uri":"https://localhost:8080/","height":35}},"source":["import tensorflow as tf\n","from tensorflow import keras\n","print(tf.__version__)\n","\n","import os\n","import time\n","from tensorflow.keras.preprocessing.sequence import pad_sequences\n","from tensorflow.keras.preprocessing.text import Tokenizer\n","import numpy as np"],"execution_count":0,"outputs":[{"output_type":"stream","text":["2.2.0-rc2\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"iGONgQeg_fmz","colab_type":"text"},"source":["# get dataset"]},{"cell_type":"code","metadata":{"id":"BlOGMTgF_hDt","colab_type":"code","colab":{}},"source":["path_to_file = tf.keras.utils.get_file('shakespeare.txt', 'https://storage.googleapis.com/download.tensorflow.org/data/shakespeare.txt')"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"oZnd9mYW_qRV","colab_type":"text"},"source":["# view data"]},{"cell_type":"code","metadata":{"id":"GCy81oaG_0os","colab_type":"code","outputId":"794f573f-3669-40e4-bf3e-2f50782145ca","executionInfo":{"status":"ok","timestamp":1586264102139,"user_tz":240,"elapsed":1915,"user":{"displayName":"Enzo Vernon","photoUrl":"","userId":"09908679712030615005"}},"colab":{"base_uri":"https://localhost:8080/","height":35}},"source":["# Read, then decode for py2 compat.\n","text = open(path_to_file, 'rb').read().decode(encoding='utf-8')\n","# length of text is the number of characters in it\n","print ('Length of text: {} characters'.format(len(text)))"],"execution_count":0,"outputs":[{"output_type":"stream","text":["Length of text: 1115394 characters\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"ymAHOep5AYhA","colab_type":"code","outputId":"0fc54186-da58-4d24-afdd-5bf444d18c19","executionInfo":{"status":"ok","timestamp":1586264102139,"user_tz":240,"elapsed":1840,"user":{"displayName":"Enzo Vernon","photoUrl":"","userId":"09908679712030615005"}},"colab":{"base_uri":"https://localhost:8080/","height":294}},"source":["# [:250]= 0 to 249 (first 250 characters)\n","print('first 250 chars: \\n{}'.format(text[:250]))"],"execution_count":0,"outputs":[{"output_type":"stream","text":["first 250 chars: \n","First Citizen:\n","Before we proceed any further, hear me speak.\n","\n","All:\n","Speak, speak.\n","\n","First Citizen:\n","You are all resolved rather to die than to famish?\n","\n","All:\n","Resolved. resolved.\n","\n","First Citizen:\n","First, you know Caius Marcius is chief enemy to the people.\n","\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"Afysf3H9An4P","colab_type":"code","outputId":"836e9d0d-fcc6-44ce-e3c6-4b002cca1e32","executionInfo":{"status":"ok","timestamp":1586264102140,"user_tz":240,"elapsed":1830,"user":{"displayName":"Enzo Vernon","photoUrl":"","userId":"09908679712030615005"}},"colab":{"base_uri":"https://localhost:8080/","height":35}},"source":["# get unique characters\n","vocab = sorted(set(text))\n","print('unique_chars: {}'.format(len(vocab)))"],"execution_count":0,"outputs":[{"output_type":"stream","text":["unique_chars: 65\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"i0Wv4zUQA_bP","colab_type":"text"},"source":["# preprocess "]},{"cell_type":"markdown","metadata":{"id":"Plpx__iUBEDo","colab_type":"text"},"source":["### Vectorize the text\n","\n","Before training, we need to map strings to a numerical representation. Create two lookup tables: one mapping characters to numbers, and another for numbers to characters."]},{"cell_type":"code","metadata":{"id":"JfbbB1qzBTFT","colab_type":"code","outputId":"ccbe3801-20a1-4e15-aba7-b2bfa30e3ba1","executionInfo":{"status":"ok","timestamp":1586264102605,"user_tz":240,"elapsed":2193,"user":{"displayName":"Enzo Vernon","photoUrl":"","userId":"09908679712030615005"}},"colab":{"base_uri":"https://localhost:8080/","height":415}},"source":["# create a mapping from unique \n","char2idx = {u:i for i,u in enumerate(vocab)}\n","idx2char = np.array(vocab)\n","\n","text_as_int = np.array([char2idx[c] for c in text])\n","\n","# Now we have an integer representation for each character. Notice that we mapped the character as indexes from 0 to `len(unique)`.\n","print('{')\n","for char,_ in zip(char2idx, range(20)):\n","    print('  {:4s}: {:3d},'.format(repr(char), char2idx[char]))\n","print('  ...\\n}')\n"],"execution_count":0,"outputs":[{"output_type":"stream","text":["{\n","  '\\n':   0,\n","  ' ' :   1,\n","  '!' :   2,\n","  '$' :   3,\n","  '&' :   4,\n","  \"'\" :   5,\n","  ',' :   6,\n","  '-' :   7,\n","  '.' :   8,\n","  '3' :   9,\n","  ':' :  10,\n","  ';' :  11,\n","  '?' :  12,\n","  'A' :  13,\n","  'B' :  14,\n","  'C' :  15,\n","  'D' :  16,\n","  'E' :  17,\n","  'F' :  18,\n","  'G' :  19,\n","  ...\n","}\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"dHmj8fmLCHY1","colab_type":"code","outputId":"1be84054-864f-413f-c8bb-07fb1b6ef05c","executionInfo":{"status":"ok","timestamp":1586264102607,"user_tz":240,"elapsed":2162,"user":{"displayName":"Enzo Vernon","photoUrl":"","userId":"09908679712030615005"}},"colab":{"base_uri":"https://localhost:8080/","height":35}},"source":["# Show how the first 13 characters from the text are mapped to integers\n","print ('{} ---- characters mapped to int ---- > {}'.format(repr(text[:13]), text_as_int[:13]))"],"execution_count":0,"outputs":[{"output_type":"stream","text":["'First Citizen' ---- characters mapped to int ---- > [18 47 56 57 58  1 15 47 58 47 64 43 52]\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"bzubH-YdE3FB","colab_type":"text"},"source":["# split dataset into training & testing \n","\n","Next divide the text into example sequences. Each input sequence will contain `seq_length` characters from the text.\n","\n","For each input sequence, the corresponding targets contain the same length of text, except shifted one character to the right.\n","\n","So break the text into chunks of `seq_length+1`."]},{"cell_type":"code","metadata":{"id":"LXBHBH3AFjzt","colab_type":"code","outputId":"812566ae-deb1-4e66-f98e-784991b782fd","executionInfo":{"status":"ok","timestamp":1586264102608,"user_tz":240,"elapsed":2125,"user":{"displayName":"Enzo Vernon","photoUrl":"","userId":"09908679712030615005"}},"colab":{"base_uri":"https://localhost:8080/","height":104}},"source":["# The maximum length sentence we want for a single input in characters\n","seq_length = 100\n","examples_per_epoch = len(text) // (seq_length+1)\n","\n","# use the `tf.data.Dataset.from_tensor_slices` function to convert the text vector into a stream of character indices\n","char_dataset = tf.data.Dataset.from_tensor_slices(text_as_int)\n","\n","for i in char_dataset.take(5):\n","  print(idx2char[i.numpy()])"],"execution_count":0,"outputs":[{"output_type":"stream","text":["F\n","i\n","r\n","s\n","t\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"V4xh6U4ZYcQT","colab_type":"code","outputId":"fffcf2d2-fc99-41c9-f5b0-da7afd0ca107","executionInfo":{"status":"ok","timestamp":1586264102609,"user_tz":240,"elapsed":2073,"user":{"displayName":"Enzo Vernon","photoUrl":"","userId":"09908679712030615005"}},"colab":{"base_uri":"https://localhost:8080/","height":104}},"source":["# The `batch` method lets us easily convert these individual characters to sequences of the desired size.\n","sequences = char_dataset.batch(seq_length+1, drop_remainder=True)\n","\n","for item in sequences.take(5):\n","  print(repr(''.join(idx2char[item.numpy()])))"],"execution_count":0,"outputs":[{"output_type":"stream","text":["'First Citizen:\\nBefore we proceed any further, hear me speak.\\n\\nAll:\\nSpeak, speak.\\n\\nFirst Citizen:\\nYou '\n","'are all resolved rather to die than to famish?\\n\\nAll:\\nResolved. resolved.\\n\\nFirst Citizen:\\nFirst, you k'\n","\"now Caius Marcius is chief enemy to the people.\\n\\nAll:\\nWe know't, we know't.\\n\\nFirst Citizen:\\nLet us ki\"\n","\"ll him, and we'll have corn at our own price.\\nIs't a verdict?\\n\\nAll:\\nNo more talking on't; let it be d\"\n","'one: away, away!\\n\\nSecond Citizen:\\nOne word, good citizens.\\n\\nFirst Citizen:\\nWe are accounted poor citi'\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"BT6qFuMOZJdp","colab_type":"code","outputId":"7ae165d5-1d7f-4f1a-a3dd-0480b5e64a80","executionInfo":{"status":"ok","timestamp":1586264102610,"user_tz":240,"elapsed":2057,"user":{"displayName":"Enzo Vernon","photoUrl":"","userId":"09908679712030615005"}},"colab":{"base_uri":"https://localhost:8080/","height":52}},"source":["# For each sequence, duplicate and shift it to form the input and target text\n","# use the `map` method to apply a simple function to each batch\n","\n","def split_input_target(chunk):\n","    input_text = chunk[:-1]\n","    target_text = chunk[1:]\n","    return input_text, target_text\n","\n","dataset = sequences.map(split_input_target)\n","\n","for input_example, target_example in  dataset.take(1):\n","  print ('Input data: ', repr(''.join(idx2char[input_example.numpy()])))\n","  print ('Target data:', repr(''.join(idx2char[target_example.numpy()])))"],"execution_count":0,"outputs":[{"output_type":"stream","text":["Input data:  'First Citizen:\\nBefore we proceed any further, hear me speak.\\n\\nAll:\\nSpeak, speak.\\n\\nFirst Citizen:\\nYou'\n","Target data: 'irst Citizen:\\nBefore we proceed any further, hear me speak.\\n\\nAll:\\nSpeak, speak.\\n\\nFirst Citizen:\\nYou '\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"b423EALCZuiY","colab_type":"text"},"source":["Each index of these vectors are processed as one time step. For the input at time step 0, the model receives the index for \"F\" and trys to predict the index for \"i\" as the next character. \n","\n","At the next timestep, it does the same thing but:\n","\n","# the `RNN` considers the previous step context in addition to the current input character."]},{"cell_type":"code","metadata":{"id":"xbR_ZBwcaXub","colab_type":"code","outputId":"3c64746f-310c-43f6-c4f1-4afd0dafdb59","executionInfo":{"status":"ok","timestamp":1586264102610,"user_tz":240,"elapsed":2047,"user":{"displayName":"Enzo Vernon","photoUrl":"","userId":"09908679712030615005"}},"colab":{"base_uri":"https://localhost:8080/","height":276}},"source":["for i, (input_idx, target_idx) in enumerate(zip(input_example[:5], target_example[:5])):\n","    print(\"Step {:4d}\".format(i))\n","    print(\"  input: {} ({:s})\".format(input_idx, repr(idx2char[input_idx])))\n","    print(\"  expected output: {} ({:s})\".format(target_idx, repr(idx2char[target_idx])))"],"execution_count":0,"outputs":[{"output_type":"stream","text":["Step    0\n","  input: 18 ('F')\n","  expected output: 47 ('i')\n","Step    1\n","  input: 47 ('i')\n","  expected output: 56 ('r')\n","Step    2\n","  input: 56 ('r')\n","  expected output: 57 ('s')\n","Step    3\n","  input: 57 ('s')\n","  expected output: 58 ('t')\n","Step    4\n","  input: 58 ('t')\n","  expected output: 1 (' ')\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"4cEBnKJfcXo2","colab_type":"code","outputId":"208ed7ad-ac90-45fd-b3f9-2b7a7f97be61","executionInfo":{"status":"ok","timestamp":1586264102610,"user_tz":240,"elapsed":2035,"user":{"displayName":"Enzo Vernon","photoUrl":"","userId":"09908679712030615005"}},"colab":{"base_uri":"https://localhost:8080/","height":35}},"source":["# generate training dataset\n","BATCH_SIZE = 64\n","BUFFER_SIZE = 10000\n","\n","# before feeding this data into the model, we need to shuffle the data and pack it into batches.\n","dataset = dataset.shuffle(BUFFER_SIZE).batch(BATCH_SIZE, drop_remainder=True)\n","\n","dataset"],"execution_count":0,"outputs":[{"output_type":"execute_result","data":{"text/plain":["<BatchDataset shapes: ((64, 100), (64, 100)), types: (tf.int64, tf.int64)>"]},"metadata":{"tags":[]},"execution_count":40}]},{"cell_type":"markdown","metadata":{"id":"V3pEhdiDccFP","colab_type":"text"},"source":["# setup hyperparameters"]},{"cell_type":"code","metadata":{"id":"O9Sp-q5AcfPt","colab_type":"code","colab":{}},"source":["vocab_size = len(vocab) # char length\n","embedding_dim = 256 # output of Embedding input layer\n","rnn_units = 1024 # num_neurons in LSTM layer\n"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"k6L1ZBlOas6V","colab_type":"text"},"source":["# build model\n","\n","\n","* `tf.keras.layers.Embedding`: The input layer. A trainable lookup table that will map the numbers of each character to a vector with `embedding_dim` dimensions;\n","* `tf.keras.layers.GRU`: A type of RNN with size `units=rnn_units` (You can also use a LSTM layer here.)\n","* `tf.keras.layers.Dense`: The output layer, with `vocab_size` outputs."]},{"cell_type":"code","metadata":{"id":"Sfz8ByOnbrWC","colab_type":"code","outputId":"1ae1233a-3b2c-49a8-f448-ba6056ce62d5","executionInfo":{"status":"ok","timestamp":1586264102611,"user_tz":240,"elapsed":2011,"user":{"displayName":"Enzo Vernon","photoUrl":"","userId":"09908679712030615005"}},"colab":{"base_uri":"https://localhost:8080/","height":259}},"source":["def build_model(vocab_size, embedding_dim, rnn_units, batch_size):\n","  model = keras.Sequential([\n","    keras.layers.Embedding(input_dim=vocab_size, output_dim=embedding_dim, batch_input_shape=[batch_size, None]),\n","    keras.layers.GRU(units=rnn_units, return_sequences=True, stateful=True, recurrent_initializer='glorot_uniform'),\n","    keras.layers.Dense(units=vocab_size)\n","  ])\n","  return model\n","\n","# instantiate model\n","model = build_model(\n","  vocab_size = len(vocab),\n","  embedding_dim=embedding_dim,\n","  rnn_units=rnn_units,\n","  batch_size=BATCH_SIZE)\n","\n","model.summary()"],"execution_count":0,"outputs":[{"output_type":"stream","text":["Model: \"sequential_3\"\n","_________________________________________________________________\n","Layer (type)                 Output Shape              Param #   \n","=================================================================\n","embedding_3 (Embedding)      (64, None, 256)           16640     \n","_________________________________________________________________\n","gru_3 (GRU)                  (64, None, 1024)          3938304   \n","_________________________________________________________________\n","dense_3 (Dense)              (64, None, 65)            66625     \n","=================================================================\n","Total params: 4,021,569\n","Trainable params: 4,021,569\n","Non-trainable params: 0\n","_________________________________________________________________\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"AnLYrd3bdgsD","colab_type":"text"},"source":["For each character the model looks up the embedding, runs the GRU one timestep with the embedding as input, and applies the dense layer to generate logits predicting the log-likelihood of the next character:\n","\n","![A drawing of the data passing through the model](https://github.com/tensorflow/docs/blob/master/site/en/tutorials/text/images/text_generation_training.png?raw=1)"]},{"cell_type":"markdown","metadata":{"id":"kKR9MDhKdrWL","colab_type":"text"},"source":["# validate model\n","\n","To get actual predictions from the model we need to sample from the output distribution, to get actual character indices. This distribution is defined by the logits over the character vocabulary.\n","\n","Note: It is important to _sample_ from this distribution as taking the _argmax_ of the distribution can easily get the model stuck in a loop.\n"]},{"cell_type":"code","metadata":{"id":"tckyJgH6d65B","colab_type":"code","outputId":"f0fa325f-e9ab-40a1-df0d-053286024e1d","executionInfo":{"status":"ok","timestamp":1586264104088,"user_tz":240,"elapsed":3476,"user":{"displayName":"Enzo Vernon","photoUrl":"","userId":"09908679712030615005"}},"colab":{"base_uri":"https://localhost:8080/","height":35}},"source":["for input_example_batch, target_example_batch in dataset.take(1):\n","  example_batch_predictions = model(input_example_batch)\n","  print(example_batch_predictions.shape, \"# (batch_size, sequence_length, vocab_size)\")\n","\n","sampled_indices = tf.random.categorical(example_batch_predictions[0], num_samples=1)\n","sampled_indices = tf.squeeze(sampled_indices,axis=-1).numpy()"],"execution_count":0,"outputs":[{"output_type":"stream","text":["(64, 100, 65) # (batch_size, sequence_length, vocab_size)\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"XOdpACJXeXTs","colab_type":"text"},"source":["This gives us, at each timestep, a prediction of the next character index:"]},{"cell_type":"code","metadata":{"id":"o43ceLwegxtb","colab_type":"code","outputId":"4026f60d-247b-45f1-ae26-2331f2315109","executionInfo":{"status":"ok","timestamp":1586264104089,"user_tz":240,"elapsed":3467,"user":{"displayName":"Enzo Vernon","photoUrl":"","userId":"09908679712030615005"}},"colab":{"base_uri":"https://localhost:8080/","height":121}},"source":["sampled_indices"],"execution_count":0,"outputs":[{"output_type":"execute_result","data":{"text/plain":["array([19, 64,  2, 46, 36, 41, 58, 40, 27, 62, 28, 62, 30, 37, 41, 37, 53,\n","       27, 28, 25, 38, 21, 50, 17, 34, 26, 49, 16, 61, 35, 24, 24, 33, 14,\n","       64, 26, 26,  0, 20, 11, 25, 64, 53, 56, 61,  4, 11,  3, 39, 35, 16,\n","       53, 42, 31, 12, 43,  3,  9, 21, 19, 33, 58, 24, 42, 47, 60, 24, 17,\n","       10, 36, 10,  4, 42, 59, 55, 40, 16, 49, 13, 59, 39, 56, 34, 35, 23,\n","       64,  4, 33, 47,  7,  4, 33, 23,  7, 31, 35, 30, 45, 11,  0])"]},"metadata":{"tags":[]},"execution_count":44}]},{"cell_type":"code","metadata":{"id":"ATAldV1Sg30X","colab_type":"code","outputId":"bbd6e2f0-1acc-49a2-f24b-28444a44bab4","executionInfo":{"status":"ok","timestamp":1586264104090,"user_tz":240,"elapsed":3414,"user":{"displayName":"Enzo Vernon","photoUrl":"","userId":"09908679712030615005"}},"colab":{"base_uri":"https://localhost:8080/","height":104}},"source":["# decode \n","print(\"Input: \\n\", repr(\"\".join(idx2char[input_example_batch[0]])))\n","print()\n","print(\"Next Char Predictions: \\n\", repr(\"\".join(idx2char[sampled_indices ])))"],"execution_count":0,"outputs":[{"output_type":"stream","text":["Input: \n"," \"t,\\nThat runaway's eyes may wink and Romeo\\nLeap to these arms, untalk'd of and unseen.\\nLovers can see\"\n","\n","Next Char Predictions: \n"," 'Gz!hXctbOxPxRYcYoOPMZIlEVNkDwWLLUBzNN\\nH;Mzorw&;$aWDodS?e$3IGUtLdivLE:X:&duqbDkAuarVWKz&Ui-&UK-SWRg;\\n'\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"Kawzf2w5jgTR","colab_type":"text"},"source":["# compile the model"]},{"cell_type":"code","metadata":{"id":"2dgxG04Ki1io","colab_type":"code","outputId":"67f9c859-375b-4013-ff22-ab167868e002","executionInfo":{"status":"ok","timestamp":1586264104090,"user_tz":240,"elapsed":3379,"user":{"displayName":"Enzo Vernon","photoUrl":"","userId":"09908679712030615005"}},"colab":{"base_uri":"https://localhost:8080/","height":52}},"source":["# The standard `tf.keras.losses.sparse_categorical_crossentropy` loss function works in this case\n","  # it is applied across the last dimension of the predictions.\n","\n","def loss(labels, logits):\n","  return keras.losses.sparse_categorical_crossentropy(y_true=labels, y_pred=logits, from_logits=True)\n","\n","example_batch_loss = loss(target_example_batch, example_batch_predictions)\n","print(\"Prediction shape: \", example_batch_predictions.shape, \" # (batch_size, sequence_length, vocab_size)\")\n","print(\"scalar_loss:      \", example_batch_loss.numpy().mean())\n","\n","model.compile(loss=loss, optimizer='adam')"],"execution_count":0,"outputs":[{"output_type":"stream","text":["Prediction shape:  (64, 100, 65)  # (batch_size, sequence_length, vocab_size)\n","scalar_loss:       4.1736493\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"179hDd6og7cQ","colab_type":"text"},"source":["# train the model\n","\n","Given the previous RNN state, and the input this time step, predict the class of the next character.\n","\n","fit the model to train & learn the optimized weights/relationships"]},{"cell_type":"code","metadata":{"id":"4i5AsY2wi_2v","colab_type":"code","colab":{}},"source":["# setup callbacks: checkpoint saving\n","checkpoint_dir = './training_checkpoints'\n","\n","# checkpoint file name\n","checkpoint_prefix = os.path.join(checkpoint_dir, 'chpt_{epoch}')\n","\n","# Use a `tf.keras.callbacks.ModelCheckpoint` to ensure that checkpoints are saved during training:\n","callbacks = keras.callbacks.ModelCheckpoint(filepath=checkpoint_prefix, save_weights_only=True)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"-o-oiYffkq4Z","colab_type":"code","outputId":"7cd1a305-c130-4299-f039-7ed5baa6dfec","executionInfo":{"status":"ok","timestamp":1586264899740,"user_tz":240,"elapsed":432870,"user":{"displayName":"Enzo Vernon","photoUrl":"","userId":"09908679712030615005"}},"colab":{"base_uri":"https://localhost:8080/","height":1000}},"source":["NUM_EPOCHS = 30\n","\n","# assign trainined model to a history var for performance querying\n","history = model.fit(dataset, epochs=NUM_EPOCHS, callbacks=[callbacks])"],"execution_count":0,"outputs":[{"output_type":"stream","text":["Epoch 1/30\n","172/172 [==============================] - 25s 144ms/step - loss: 2.6831\n","Epoch 2/30\n","172/172 [==============================] - 25s 144ms/step - loss: 1.9601\n","Epoch 3/30\n","172/172 [==============================] - 24s 141ms/step - loss: 1.6916\n","Epoch 4/30\n","172/172 [==============================] - 25s 147ms/step - loss: 1.5436\n","Epoch 5/30\n","172/172 [==============================] - 25s 147ms/step - loss: 1.4548\n","Epoch 6/30\n","172/172 [==============================] - 25s 146ms/step - loss: 1.3945\n","Epoch 7/30\n","172/172 [==============================] - 25s 143ms/step - loss: 1.3476\n","Epoch 8/30\n","172/172 [==============================] - 25s 144ms/step - loss: 1.3094\n","Epoch 9/30\n","172/172 [==============================] - 25s 147ms/step - loss: 1.2739\n","Epoch 10/30\n","172/172 [==============================] - 25s 145ms/step - loss: 1.2411\n","Epoch 11/30\n","172/172 [==============================] - 25s 147ms/step - loss: 1.2083\n","Epoch 12/30\n","172/172 [==============================] - 25s 145ms/step - loss: 1.1748\n","Epoch 13/30\n","172/172 [==============================] - 25s 143ms/step - loss: 1.1413\n","Epoch 14/30\n","172/172 [==============================] - 25s 146ms/step - loss: 1.1067\n","Epoch 15/30\n","172/172 [==============================] - 26s 149ms/step - loss: 1.0727\n","Epoch 16/30\n","172/172 [==============================] - 25s 144ms/step - loss: 1.0360\n","Epoch 17/30\n","172/172 [==============================] - 26s 149ms/step - loss: 0.9988\n","Epoch 18/30\n","172/172 [==============================] - 25s 146ms/step - loss: 0.9647\n","Epoch 19/30\n","172/172 [==============================] - 25s 146ms/step - loss: 0.9285\n","Epoch 20/30\n","172/172 [==============================] - 25s 147ms/step - loss: 0.8946\n","Epoch 21/30\n","172/172 [==============================] - 25s 144ms/step - loss: 0.8634\n","Epoch 22/30\n","172/172 [==============================] - 25s 145ms/step - loss: 0.8346\n","Epoch 23/30\n","172/172 [==============================] - 25s 146ms/step - loss: 0.8084\n","Epoch 24/30\n","172/172 [==============================] - 25s 145ms/step - loss: 0.7836\n","Epoch 25/30\n","172/172 [==============================] - 25s 146ms/step - loss: 0.7641\n","Epoch 26/30\n","172/172 [==============================] - 25s 148ms/step - loss: 0.7435\n","Epoch 27/30\n","172/172 [==============================] - 25s 143ms/step - loss: 0.7308\n","Epoch 28/30\n","172/172 [==============================] - 25s 147ms/step - loss: 0.7154\n","Epoch 29/30\n","172/172 [==============================] - 26s 151ms/step - loss: 0.7066\n","Epoch 30/30\n","172/172 [==============================] - 26s 149ms/step - loss: 0.6934\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"tKqcUIg2lKnP","colab_type":"text"},"source":["# evaluate model: \n","\n","### `generating text using the learned model`\n","\n","---\n","\n","Because of the way the RNN state is passed from timestep to timestep, the model only accepts a fixed batch size once built.\n","\n","To run the model with a different `batch_size`, we need to rebuild the model and restore the weights from the checkpoint.\n"]},{"cell_type":"code","metadata":{"id":"W3eNoQJglsZL","colab_type":"code","outputId":"21281898-2a46-46a6-cda9-fbedb44b2373","executionInfo":{"status":"ok","timestamp":1586264900036,"user_tz":240,"elapsed":309,"user":{"displayName":"Enzo Vernon","photoUrl":"","userId":"09908679712030615005"}},"colab":{"base_uri":"https://localhost:8080/","height":259}},"source":["tf.train.latest_checkpoint(checkpoint_dir)\n","\n","# To keep this prediction step simple, use a batch size of 1.\n","model = build_model(vocab_size, embedding_dim, rnn_units, batch_size=1)\n","\n","model.load_weights(tf.train.latest_checkpoint(checkpoint_dir))\n","model.build(tf.TensorShape([1, None]))\n","\n","model.summary()"],"execution_count":0,"outputs":[{"output_type":"stream","text":["Model: \"sequential_4\"\n","_________________________________________________________________\n","Layer (type)                 Output Shape              Param #   \n","=================================================================\n","embedding_4 (Embedding)      (1, None, 256)            16640     \n","_________________________________________________________________\n","gru_4 (GRU)                  (1, None, 1024)           3938304   \n","_________________________________________________________________\n","dense_4 (Dense)              (1, None, 65)             66625     \n","=================================================================\n","Total params: 4,021,569\n","Trainable params: 4,021,569\n","Non-trainable params: 0\n","_________________________________________________________________\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"3ldW2ka9mO5y","colab_type":"text"},"source":["### The prediction loop\n","\n","The following code block generates the text:\n","\n","* It Starts by choosing a start string, initializing the RNN state and setting the number of characters to generate.\n","\n","* Get the prediction distribution of the next character using the start string and the RNN state.\n","\n","* Then, use a categorical distribution to calculate the index of the predicted character. Use this predicted character as our next input to the model.\n","\n","* The RNN state returned by the model is fed back into the model so that it now has more context, instead than only one character. After predicting the next character, the modified RNN states are again fed back into the model, which is how it learns as it gets more context from the previously predicted characters.\n","\n","Looking at the generated text, you'll see the model knows when to capitalize, make paragraphs and imitates a Shakespeare-like writing vocabulary. With the small number of training epochs, it has not yet learned to form coherent sentences.\n","\n","![To generate text the model's output is fed back to the input](https://github.com/tensorflow/docs/blob/master/site/en/tutorials/text/images/text_generation_sampling.png?raw=1)\n"]},{"cell_type":"code","metadata":{"id":"k8dF5bzMmqBd","colab_type":"code","colab":{}},"source":["def generate_text(model, start_string):\n","  # num characters to generate \n","  num_generate = 1000\n","\n","  # tokenize/vectorize= transform starting string to numbers\n","  input_eval = [char2idx[s] for s in start_string]\n","  input_eval = tf.expand_dims(input_eval, 0)\n","\n","  # define default text list\n","  text_generated = []\n","  # low=predictable/high=suprise\n","  temperature = 1.0 \n","\n","  # batch_size == 1\n","  model.reset_states()\n","  for i in range(num_generate):\n","    predictions = model(input_eval)\n","    # remove batch dim\n","    predictions = tf.squeeze(predictions, 0)\n","    # using a categorical distribution to predict the character returned by the model\n","    predictions = predictions / temperature\n","    predicted_id = tf.random.categorical(predictions, num_samples=1)[-1, 0].numpy()\n","\n","    # We pass the predicted character as the next input to the model\n","    # along with the previous hidden state\n","    input_eval = tf.expand_dims([predicted_id], 0)\n","    text_generated.append(idx2char[predicted_id])\n","  return (start_string+''.join(text_generated))"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"HEmrWwjEqPKt","colab_type":"code","outputId":"73b262a4-c63f-46bd-d4f7-c65018b027ea","executionInfo":{"status":"ok","timestamp":1586264907011,"user_tz":240,"elapsed":6978,"user":{"displayName":"Enzo Vernon","photoUrl":"","userId":"09908679712030615005"}},"colab":{"base_uri":"https://localhost:8080/","height":622}},"source":["print(generate_text(model, start_string=u\"ROMEO: \"))"],"execution_count":0,"outputs":[{"output_type":"stream","text":["ROMEO: I beseech you, they\n","ANTELIO:\n","Let me pass alack, and underetter tailors,\n","Suppose thy neck to vent our sails that would say,\n","I would have seen them datch, and relies to Rome,\n","Who hath destroy'd of?\n","\n","POMPEY:\n","Pompey, you shall bear my sense.\n","If further when I have said 'tis charity.\n","\n","FROTH:\n","I think, if it be so, then, at lent\n","Allays, follows, keep home\n","Do make thy sea-swain, wilt thou, Northumberland,\n","When ne'er shall be the king's King Baptists speak for him!\n","I shall rent me to the under 'ma.\n","Your first order, I will temper it.\n","\n","AUFIDIUS:\n","And pale were virtues ng in mine own.\n","RY VI\n","Come, cords, coming, with my soldiers,\n","You that have turn you:\n","Forsooth you where you are? Which, as\n","I had an Edward to learn, and for line own intent\n","That he shall serve for him: go day: go; pardon, an't like a light.\n","\n","BAPTISTA:\n","Let mine condialemous to his praise.\n","\n","PRINCE EDWARD:\n","Return an ourself tells me, for even thus we parted withal, I know had thou, the beat be\n","The traitor Angelo, a poor deputy--\n","And th\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"wC3fRO8cqcPn","colab_type":"text"},"source":["# advanced training\n","\n","We will use `tf.GradientTape` to track the gradients. You can learn more about this approach by reading the [eager execution guide](https://www.tensorflow.org/guide/eager).\n","\n","The procedure works as follows:\n","\n","* First, initialize the RNN state. We do this by calling the `tf.keras.Model.reset_states` method.\n","\n","* Next, iterate over the dataset (batch by batch) and calculate the *predictions* associated with each.\n","\n","* Open a `tf.GradientTape`, and calculate the predictions and loss in that context.\n","\n","* Calculate the gradients of the loss with respect to the model variables using the `tf.GradientTape.grads` method.\n","\n","* Finally, take a step downwards by using the optimizer's `tf.train.Optimizer.apply_gradients` method.\n"]},{"cell_type":"code","metadata":{"id":"YCTu40yyqllR","colab_type":"code","colab":{}},"source":["model = build_model(\n","  vocab_size = len(vocab),\n","  embedding_dim=embedding_dim,\n","  rnn_units=rnn_units,\n","  batch_size=BATCH_SIZE)\n","\n","optimizer = tf.keras.optimizers.Adam()"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"UUFfw_Rxq7D5","colab_type":"code","colab":{}},"source":["@tf.function\n","def train_step(inp, target):\n","  with tf.GradientTape() as tape:\n","    predictions = model(inp)\n","    loss = tf.reduce_mean(\n","        tf.keras.losses.sparse_categorical_crossentropy(\n","            target, predictions, from_logits=True))\n","  grads = tape.gradient(loss, model.trainable_variables)\n","  optimizer.apply_gradients(zip(grads, model.trainable_variables))\n","\n","  return loss"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"OgEYJ8EMq_lM","colab_type":"code","outputId":"3bdf8e88-b951-45cd-af92-17bcd0ebe160","executionInfo":{"status":"ok","timestamp":1586265161432,"user_tz":240,"elapsed":254424,"user":{"displayName":"Enzo Vernon","photoUrl":"","userId":"09908679712030615005"}},"colab":{"base_uri":"https://localhost:8080/","height":881}},"source":["# Training step\n","EPOCHS = 10\n","\n","for epoch in range(EPOCHS):\n","  start = time.time()\n","\n","  # initializing the hidden state at the start of every epoch\n","  # initally hidden is None\n","  hidden = model.reset_states()\n","\n","  for (batch_n, (inp, target)) in enumerate(dataset):\n","    loss = train_step(inp, target)\n","\n","    if batch_n % 100 == 0:\n","      template = 'Epoch {} Batch {} Loss {}'\n","      print(template.format(epoch+1, batch_n, loss))\n","\n","  # saving (checkpoint) the model every 5 epochs\n","  if (epoch + 1) % 5 == 0:\n","    model.save_weights(checkpoint_prefix.format(epoch=epoch))\n","\n","  print ('Epoch {} Loss {:.4f}'.format(epoch+1, loss))\n","  print ('Time taken for 1 epoch {} sec\\n'.format(time.time() - start))\n","\n","model.save_weights(checkpoint_prefix.format(epoch=epoch))"],"execution_count":0,"outputs":[{"output_type":"stream","text":["Epoch 1 Batch 0 Loss 4.173681259155273\n","Epoch 1 Batch 100 Loss 2.3300321102142334\n","Epoch 1 Loss 2.1038\n","Time taken for 1 epoch 26.867459535598755 sec\n","\n","Epoch 2 Batch 0 Loss 2.1020798683166504\n","Epoch 2 Batch 100 Loss 1.894805908203125\n","Epoch 2 Loss 1.7698\n","Time taken for 1 epoch 25.631630182266235 sec\n","\n","Epoch 3 Batch 0 Loss 1.7947996854782104\n","Epoch 3 Batch 100 Loss 1.6558419466018677\n","Epoch 3 Loss 1.6081\n","Time taken for 1 epoch 24.796597242355347 sec\n","\n","Epoch 4 Batch 0 Loss 1.5326881408691406\n","Epoch 4 Batch 100 Loss 1.508499264717102\n","Epoch 4 Loss 1.4955\n","Time taken for 1 epoch 24.957431316375732 sec\n","\n","Epoch 5 Batch 0 Loss 1.4661144018173218\n","Epoch 5 Batch 100 Loss 1.4418679475784302\n","Epoch 5 Loss 1.4683\n","Time taken for 1 epoch 25.24788498878479 sec\n","\n","Epoch 6 Batch 0 Loss 1.3600000143051147\n","Epoch 6 Batch 100 Loss 1.3908978700637817\n","Epoch 6 Loss 1.4183\n","Time taken for 1 epoch 25.38266658782959 sec\n","\n","Epoch 7 Batch 0 Loss 1.3258235454559326\n","Epoch 7 Batch 100 Loss 1.3791674375534058\n","Epoch 7 Loss 1.3506\n","Time taken for 1 epoch 25.216983795166016 sec\n","\n","Epoch 8 Batch 0 Loss 1.275151252746582\n","Epoch 8 Batch 100 Loss 1.3016492128372192\n","Epoch 8 Loss 1.3179\n","Time taken for 1 epoch 25.150683641433716 sec\n","\n","Epoch 9 Batch 0 Loss 1.2274487018585205\n","Epoch 9 Batch 100 Loss 1.2513221502304077\n","Epoch 9 Loss 1.2757\n","Time taken for 1 epoch 25.08799719810486 sec\n","\n","Epoch 10 Batch 0 Loss 1.1978182792663574\n","Epoch 10 Batch 100 Loss 1.2352187633514404\n","Epoch 10 Loss 1.2160\n","Time taken for 1 epoch 25.35469150543213 sec\n","\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"fRBpnPoFrM0l","colab_type":"text"},"source":["# clean up\n","\n","terminate memory kernel and free up memory resources"]},{"cell_type":"code","metadata":{"id":"K9sesBewrSYu","colab_type":"code","colab":{}},"source":["import os, signal\n","\n","os.kill(os.getpid(), signal.SIGKILL)"],"execution_count":0,"outputs":[]}]}